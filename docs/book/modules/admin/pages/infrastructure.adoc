
:sectlinks:

= Infrastructure

Trustify requires some infrastructure services for its installation. The services required are:

* OIDC provider
* PostgreSQL database instance
* Storage, either:
** Filesystem
** S3 compatible

Those services have to be provided by the user before the installation is being performed. Some information, like access
credentials, must be provided during the installation of Trustify.

There are different ways to set up such services. However, it is up to the user to provide those services and set them
up.

The following sections provide a few examples on how they can be provided in different ways. Keep in mind, those are just
examples, and you can modify them to suit your needs, or choose a different approach in providing those services.

== Self-managed Kubernetes

A simple approach is to use Keycloak as an OIDC provider, a PostgreSQL container, and a persistent volume claim for
the filesystem storage.

To set this up, it is possible to just use existing Helm charts for Keycloak and PostgreSQL. We do provide an
opinionated infrastructure Helm chart for this case at: https://github.com/trustification/trustify-helm-charts/tree/main/charts/trustify-infrastructure

You can install this using:

[source,bash]
----
NAMESPACE=trustify
APP_DOMAIN=public.cluster.domain
helm upgrade --install -n $NAMESPACE --repo https://trustification.io/trustify-helm-charts/ infrastructure trustify-infrastructure --values <values-file> --set-string keycloak.ingress.hostname=sso$APP_DOMAIN --set-string appDomain=$APP_DOMAIN
----

For this, you will need to provide a Helm "values" file. Which is a YAML file, providing additional information for
the Helm chart.

An example file, for Minikube, is:

[source,yaml]
----
keycloak:
  enabled: true
  production: false
  auth:
    adminUser: admin
    adminPassword: admin123456 # notsecret, replace
  tls:
    enabled: false
  service: {}
  ingress:
    enabled: true
    servicePort: http

oidc:
  clients:
    frontend: {}
    cli:
      clientSecret:
        value: 5460cc91-4e20-4edd-881c-b15b169f8a79 # notsecret, replace
----

=== Collecting traces and metrics

To enable observability, you must configure application-level telemetry using
command-line flags. Both metrics and traces are optional and can be enabled independently
or together. However, an OpenTelemetry Collector endpoint is required when either signal is enabled.

As Trustify do not ship with an OpenTelemetry Collector, we do provide an opinionated
infrastructure Helm chart for this case at: https://github.com/trustification/trustify-helm-charts/tree/main/charts/trustify-infrastructure

Telemetry is configured via Helm values, which are rendered as command-line flags
passed to the application on startup. Example with Minikube:

[source,bash]
----
helm upgrade --install -n $NAMESPACE \
  --repo https://trustification.io/trustify-helm-charts/ trustify trustify \
  --values values-minikube.yaml \
  --set-string appDomain=$APP_DOMAIN \
  --set tracing.enabled=true \
  --set metrics.enabled=true \
  --set-string collector.endpoint="http://infrastructure-otelcol:4317"
----

Optionally, the following OpenTelemetry settings can be customized using Helm CLI flags:

[source,bash]
----
--set metrics.otelMetricExportInterval=1000
--set tracing.otelBspMaxExportBatchSize=64
--set tracing.otelTracesSampler="traceidratio"
--set tracing.otelTracesSamplerArg=1
----

It will override the following environment variables:

* The maximum number of spans per batch, controlled by `OTEL_BSP_MAX_EXPORT_BATCH_SIZE`.
* The trace sampling strategy, set via `OTEL_TRACES_SAMPLER`.
* The trace sampling probability, configured using `OTEL_TRACES_SAMPLER_ARG`.
* The metrics export interval, which defaults to 60 seconds and can be adjusted via `OTEL_METRIC_EXPORT_INTERVAL`.

For Minikube, we are using the following default values:

[source,yaml]
----
- name: OTEL_METRIC_EXPORT_INTERVAL
  value: "60000"
- name: OTEL_BSP_MAX_EXPORT_BATCH_SIZE
  value: "32"
- name: OTEL_TRACES_SAMPLER
  value: parentbased_traceidratio
- name: OTEL_TRACES_SAMPLER_ARG
  value: "0.1"
----

== AWS services

It also is possible to use AWS managed services. The following AWS services can be used:

* OIDC provider: AWS Cognito
* PostgreSQL database instance: AWS RDS
* Storage: AWS S3

=== Manual setup

You can create the AWS resources manually, either through the AWS console or using the AWS CLI.

=== Terraform with OpenShift

Trustify also provides an example Terraform setup, which is intended to quickly deploy an opinionated set of services.
The Terraform scripts will create the AWS resources, as well as create Kubernetes resources with information from the
Terraform creation process, so that the Helm charts can pick up this information.

==== Main module

To use the Terraform scripts, you will need to create a wrapper/main module, referencing this `trustify` module.

NOTE: The following example file needs to be adapted to your needs.
Example values have to be replaced with values that suit your deployment.

[source,hcl-terraform]
----
provider "aws" {
  region  = "<your region>"  # <1>
  profile = "<your aws cli profile>" # <2>
}

provider "kubernetes" {
  config_path    = "<path to kubeconfig>" # <3>
  config_context = "<name of the kubectl context>" # <4>
}

variable "app-domain" {
  type = string
}

module "trustify" {
  source = "./trustify" # <5>

  cluster-vpc-id = "<your cluster vpc>" # <6>
  availability-zone = "<your availability zone inside your region>" # <7>

  namespace = "trustify"

  admin-email = "<your e-mail address>" # <8>
  sso-domain = "<a free cognito console domain name>" # <9>
  console-url = "https://server${var.app-domain}"
}
----
<1> The AWS region you want to create the resources in
<2> The name of the AWS CLI profile you want to use
<3> The location to the "kubeconfig" file, required for accessing the Kubernetes cluster
<4> The name of the Kubernetes client context (in the `kubeconfig`) to use
<5> The location of the `trustify` Terraform module
<6> The VPC ID of the OpenShift cluster, used to allow access to the RDS database
<7> The availability zone the RDS instance should be created in. Must be valid for the AWS region.
<8> The e-mail of the admin user, which will get frontend access to Trustification
<9> An AWS Cognito domain prefix. It is globally unique and has to be still available.

==== Creating the resources

First, initialize the OpenTofu instance.
This will set up the required providers and does not yet create any resources:

[source,bash]
----
tofu init
----

The following commands require the environment variable `APP_DOMAIN` to be set.
You can do this using the following command:

[source,bash]
----
NAMESPACE=trustify
APP_DOMAIN=-$NAMESPACE.$(kubectl -n openshift-ingress-operator get ingresscontrollers.operator.openshift.io default -o jsonpath='{.status.domain}')
----

Then, check if the resources can be created. This does not yet create the resources:

[source,bash]
----
tofu plan --var app-domain=$APP_DOMAIN
----

This will show you the resources which will get created and check if the creation is expected to be successful.

If this worked fine, proceed with actually creating the resources:

[source,bash]
----
tofu apply --var app-domain=$APP_DOMAIN
----

This will also create some resources in the Kubernetes cluster, including the credentials to the AWS accounts
created for accessing the created AWS resources.

=== Running the Helm chart

Prepare a "values" files, named `values-ocp-aws.yaml`:

[source,yaml]
----
ingress:
  className: openshift-default

authenticator:
  type: cognito

storage:
  type: s3
  region:
    valueFrom:
      configMapKeyRef:
        name: aws-storage
        key: region
  bucket: trustify
  accessKey:
    valueFrom:
      secretKeyRef:
        name: storage-credentials
        key: aws_access_key_id
  secretKey:
    valueFrom:
      secretKeyRef:
        name: storage-credentials
        key: aws_secret_access_key

database:
  host:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.host
  port:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.port
  name:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.name
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.port

createDatabase:
  name:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.name
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.password

migrateDatabase:
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.password

modules:
  createDatabase:
    enabled: true
  migrateDatabase:
    enabled: true

oidc:
  issuerUrl:
    valueFrom:
      configMapKeyRef:
        name: aws-oidc
        key: issuer-url
  clients:
    frontend:
      clientId:
        valueFrom:
          secretKeyRef:
            name: oidc-frontend
            key: client-id
    cli:
      clientId:
        valueFrom:
          secretKeyRef:
            name: oidc-cli
            key: client-id
      clientSecret:
        valueFrom:
          secretKeyRef:
            name: oidc-cli
            key: client-secret
----

You can now run the Helm chart using the following command:

[source,bash]
----
helm upgrade --install --repo https://trustification.io/trustify-helm-charts/ --devel -n $NAMESPACE trustify charts/trustify --values values-ocp-aws.yaml --set-string appDomain=$APP_DOMAIN
----

NOTE: The `--devel` flag is currently necessary as the Helm chart has a pre-release version.

== Red Hat Services on Openshift

Install the following Red Hat services:

* Red Hat Build of Keycloak (RHBK) operator as the OpenID Connect (OIDC) provider.
* Red Hat OpenShift Data Foundation operator as the storage provider.
* Red Hat Build of Opentelemetry for tracing and metrics.

=== Manual setup

=== Red Hat Build of Keycloak Operator

 * Install Red Hat Build of Keycloak Operator with deployment https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.2/html/operator_guide/installation-[guide].

==== Prerequisites
The Prerequisites for the Keycloak operator configuration are,

* Database
* Hostname
* TLS certificate and keys

[[_keycloak_postgres]]
===== Create PostgreSQL instance
* Login to `registry.redhat.io` using podman or docker cli
* Create Image pull secret for `registry.redhat.io` using the below command
+
[source,bash]
----
oc create secret generic docker-cred --from-file=.dockerconfigjson=$XDG_RUNTIME_DIR/containers/auth.json --type=kubernetes.io/dockerconfigjson
----

* Create PostgreSQL database instance for Keycloak persistent storage with the below `StatefulSet` and `Service`
+
[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-tpa
  namespace: <Installation Namespace>
spec:
  serviceName: postgres-tpa-service
  selector:
    matchLabels:
      app: postgres-tpa
  replicas: 1
  template:
    metadata:
      labels:
        app: postgres-tpa
    spec:
      imagePullSecrets:
      - name: docker-cred # Image pull secret
      containers:
        - name: postgres-tpa
          image: registry.redhat.io/rhel9/postgresql-16 # Postgres image provided by Red Hat
          volumeMounts:
            - mountPath: /data
              name: cache-volume
          env:
            - name: POSTGRESQL_USER
              value: trustify
            - name: POSTGRESQL_PASSWORD
              value: trustify1234
            - name: POSTGRESQL_DATABASE
              value: trustify
      volumes:
        - name: cache-volume
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-tpa
spec:
  selector:
    app: postgres-tpa
  type: ClusterIP
  ports:
  - port: 5432
    targetPort: 5432
----

[[_keycloak_hostname_]]
===== Hostname
* Keycloak can be exposed using OpenShift's DNS and wildcard routes. Retrieve the wild card with the below command,
+
[source, bash]
----
oc login
export NAMESPACE=<Installation Namespace>
export APP_DOMAIN=-$NAMESPACE.$(kubectl -n openshift-ingress-operator get ingresscontrollers.operator.openshift.io default -o jsonpath='{.status.domain}')
----

The value in `APP_DOMAIN` can be used for keycloak `hostname`

===== TLS certs and keys
* Retrieve the default router's TLS key and certificate, which will be used as a trust anchor for Keycloak. Then, create a Kubernetes `Secret` from these files.
+
[source,bash]
----
oc get secret -n openshift-ingress router-certs-default -o go-template='{{index .data "tls.key"}}' | base64 -d > tls.key
oc get secret -n openshift-ingress router-certs-default -o go-template='{{index .data "tls.crt"}}' | base64 -d > tls.crt
oc create secret tls my-tls-secret --cert=tls.crt --key=tls.key -n <Installation Namespace>
----
+
[NOTE]
For OCP V4.19, the secret name is `router-certs`

==== Installation
* Make sure the Red Hat Build of Keycloak Operator is installed. Refer - Deployment https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.2/html/operator_guide/installation-[guide]
* Create a `Secret` to store the PostgreSQL database <<_keycloak_postgres,credentials>> that Keycloak will use to connect to the database.
+
[source,bash]
----
oc create secret generic keycloak-db-secret --from-literal=username=trustify --from-literal=password=trustify1234 -n <Installation Namespace>
----

* Define the `Keycloak` Custom Resource (CR) that the Keycloak Operator will use to deploy and configure your Keycloak instance.
+
[source,yaml]
----
apiVersion: k8s.keycloak.org/v2alpha1
kind: Keycloak
metadata:
  name: example-kc
spec:
  instances: 1
  db:
    vendor: postgres
    usernameSecret:
      name: keycloak-db-secret
      key: username
    passwordSecret:
      name: keycloak-db-secret
      key: password
    host: postgres-tpa
    database: trustify
    port: 5432
  http:
    httpEnabled: true
    httpPort: 8180
    httpsPort: 8543
    tlsSecret: my-tls-secret
  hostname:
    hostname: https://sso<Replace with$APP_DOAMIN> # APP_DOMAIN value --> 1
    strict: false
    backchannelDynamic: true
  transaction:
    xaEnabled: false
----
+
**References:**
+
. _APP_DOMAIN value:_ Replace the hostname with the APP_DOMAIN value retrieved from <<_keycloak_hostname_,Hostname>> step. The example would be something like this,
 `hostname: https://sso-tpa.apps.ocp.cluster.net`

[[_keycloak_usage_]]
===== Usage
* Retrieve the Admin console username and password using the below commands,
+
[source, bash]
----
oc get secret example-kc-initial-admin -n $NAMESPACE -o jsonpath='{.data.username}' | base64 --decode
oc get secret example-kc-initial-admin -n $NAMESPACE -o jsonpath='{.data.password}' | base64 --decode
----
* On Openshift console, Go to Networking -> Routes
* Retrieve the Route of keycloak, Open the keycloak admin console
* Login to Keycloak admin console using the secrets retrieved above

==== Configuring Keycloak
Complete the following steps to configure Keycloak for RHTPA

[[_keycloak_bin_download]]
===== Download Keycloak
 * https://www.keycloak.org/downloads[Download] and extract Keycloak binary

[[_keycloak_clients]]
===== Keycloak Clients
 * Create two keycloak client json files `client-frontend.json` and `client-cli.json` with the below content
 ** `client-frontend.json`
+
[source, json]
----
{
  "clientId": "frontend",
  "clientAuthenticatorType": "client-secret",
  "enabled": true,
  "publicClient": true,
  "implicitFlowEnabled": true,
  "standardFlowEnabled": true,
  "directAccessGrantsEnabled": false,
  "serviceAccountsEnabled": false,
  "fullScopeAllowed": true,
  "webOrigins": [
	"*"
  ],
  "defaultClientScopes": [
	"basic",
	"email",
	"profile",
	"roles",
	"web-origins",
	"create:document",
	"read:document",
	"update:document",
	"delete:document"
  ],
  "optionalClientScopes": [
	"address",
	"microprofile-jwt",
	"offline_access",
	"phone"
  ],
  "attributes": {
	"access.token.lifespan": "300",
	"post.logout.redirect.uris": "+"
  }
}
----
 ** `client-cli.json`
+
[source, json]
----
{
  "clientId": "cli",
  "clientAuthenticatorType": "client-secret",
  "enabled": true,
  "publicClient": false,
  "implicitFlowEnabled": false,
  "standardFlowEnabled": false,
  "directAccessGrantsEnabled": false,
  "serviceAccountsEnabled": true,
  "fullScopeAllowed": true,
  "defaultClientScopes": [
	"basic",
	"email",
	"profile",
	"roles",
	"web-origins",
	"create:document",
	"read:document",
	"update:document",
	"delete:document"
  ],
  "optionalClientScopes": [
	"address",
	"microprofile-jwt",
	"offline_access",
	"phone"
  ],
  "attributes": {
	"access.token.lifespan": "300",
	"post.logout.redirect.uris": "+"
  }
}
----

[[_keycloak_config_script]]
===== Create Realm, Roles, clients, client scopes and users
* Update the below shell script with the keycloak details to create Realm, role, clients, client scopes and Users
+
++++
<details>
  <summary>Click here to show the bash script</summary>
  <pre><code class="language bash">
[source, bash]
----
#!/bin/bash

# --- Configuration Variables ---
# The path to your kcadm.sh script.
KCADM_PATH="Path/to/kcadm.sh" # ---> 1

# Keycloak server connection details
KEYCLOAK_URL="<Keycloak URL>" # ---> 2
KEYCLOAK_ADMIN="<Keycloak admin username>" # ---> 3
KEYCLOAK_ADMIN_PASSWORD="<Keycloak admin password>" # ---> 4

# Realm settings
REALM="trust"

# Role name
TRUSTD_ROLE_NAME="trustd"

# Admin username
TRUSTED_ADMIN_USERNAME="admin" # ---> 5
# Admin user password
TRUSTED_ADMIN_PASSWORD="admin123456" # ---> 6

# Directory containing client JSON files
INIT_DATA="/path/to/client/json/" # ---> 7

CLIENTS=("cli" "frontend")

# Exit on interrupt
trap break INT

while ! "$KCADM_PATH" config credentials --server "$KEYCLOAK_URL" --realm master --user "$KEYCLOAK_ADMIN" --password "$KEYCLOAK_ADMIN_PASSWORD" &> /dev/null; do
  echo "Waiting for Keycloak to start up..."
  sleep 5
done

echo "Keycloak ready"

# Create/update realm
REALM_OPTS=()
REALM_OPTS+=(-s enabled=true)
REALM_OPTS+=(-s "displayName=Trusted Content")
REALM_OPTS+=(-s registrationAllowed=true)
REALM_OPTS+=(-s resetPasswordAllowed=true)
REALM_OPTS+=(-s loginWithEmailAllowed=false)

if "$KCADM_PATH" get "realms/${REALM}" &> /dev/null; then
  # exists -> update
  "$KCADM_PATH" update "realms/${REALM}" "${REALM_OPTS[@]}"
else
  # need to create
  "$KCADM_PATH" create realms -s "realm=${REALM}" "${REALM_OPTS[@]}"
fi

# Create realm roles
"$KCADM_PATH" create roles -r "$REALM" -s name="$TRUSTD_ROLE_NAME" || true
# add TRUSTD_ROLE_NAME as default role
"$KCADM_PATH" add-roles -r "$REALM" --rname "default-roles-${REALM}" --rolename "$TRUSTD_ROLE_NAME"

MANAGER_ID=$("$KCADM_PATH" get roles -r "$REALM" --fields id,name --format csv --noquotes | grep ",$TRUSTD_ROLE_NAME" | awk -F ',' '{print $1}')

# Create scopes
# shellcheck disable=SC2043
for i in read:document; do
  "$KCADM_PATH" create client-scopes -r "$REALM" -s "name=$i" -s protocol=openid-connect || true
done

for i in create:document update:document delete:document; do
  "$KCADM_PATH" create client-scopes -r "$REALM" -s "name=$i" -s protocol=openid-connect || true
  ID=$("$KCADM_PATH" get client-scopes -r "$REALM" --fields id,name --format csv --noquotes | grep ",${i}" | awk -F ',' '{print $1}')
  # add all scopes to the TRUSTD_ROLE_NAME role
  "$KCADM_PATH" create "client-scopes/${ID}/scope-mappings/realm" -r "$REALM" -b '[{"name":"$TRUSTD_ROLE_NAME", "id":"'"${MANAGER_ID}"'"}]' || true
done

# Create and configure the cli client
for client in "${CLIENTS[@]}"; do
	ID=$("$KCADM_PATH" get clients -r "$REALM" --query exact=true --query "clientId=${client}" --fields id --format csv --noquotes)
	CLIENT_OPTS=()
	if [[ -n "$ID" ]]; then
	  # TODO: replace with update once https://github.com/keycloak/keycloak/issues/12484 is fixed
	  "$KCADM_PATH" delete "clients/${ID}" -r "$REALM"
	  "$KCADM_PATH" create clients -r "$REALM" -f "${INIT_DATA}/client-${client}.json" "${CLIENT_OPTS[@]}"
	else
	  "$KCADM_PATH" create clients -r "$REALM" -f "${INIT_DATA}/client-${client}.json" "${CLIENT_OPTS[@]}"
	fi
	# now set the client-secret
	ID=$("$KCADM_PATH" get clients -r "$REALM" --query exact=true --query "clientId=${client}" --fields id --format csv --noquotes)
	if [ "${client}" == "cli" ]; then
	  "$KCADM_PATH" add-roles -r "$REALM" --uusername service-account-${client} --rolename "$TRUSTD_ROLE_NAME"
	fi
done
# Create user
ID=$("$KCADM_PATH" get users -r "$REALM" --query exact=true --query "username=$TRUSTED_ADMIN_USERNAME" --fields id --format csv --noquotes)
# the next check might seem weird, but that's just Keycloak reporting a "user not found" in two different ways
if [[ -n "$ID" && "$ID" != "[]" ]]; then
  "$KCADM_PATH" update "users/$ID" -r "$REALM" -s enabled=true
else
  "$KCADM_PATH" create users -r "$REALM" -s "username=$TRUSTED_ADMIN_USERNAME" -s enabled=true -s email=test@example.com -s emailVerified=true -s firstName=Admin -s lastName=Admin
fi

# set role
"$KCADM_PATH" add-roles -r "$REALM" --uusername "$TRUSTED_ADMIN_USERNAME" --rolename "$TRUSTD_ROLE_NAME"

# set password
ID=$("$KCADM_PATH" get users -r "$REALM" --query exact=true --query "username=$TRUSTED_ADMIN_USERNAME" --fields id --format csv --noquotes)
"$KCADM_PATH" update "users/${ID}/reset-password" -r "$REALM" -s type=password -s "value=${TRUSTED_ADMIN_PASSWORD}" -s temporary=false -n

echo Keycloak initialization complete
----
</code></pre>
</details>
++++
+
**References:**
+
. _KCADM_PATH:_ Update the KCADM path from <<_keycloak_bin_download, downaloded>> directory
. _KEYCLOAK_URL:_ Keycloak URL from <<_keycloak_usage_,operator>>
. _KEYCLOAK_ADMIN:_ Keycloak ADMIN username from <<_keycloak_usage_,operator>>
. _KEYCLOAK_ADMIN_PASSWORD:_ Keycloak ADMIN password from <<_keycloak_usage_,operator>>
. _TRUSTED_ADMIN_USERNAME:_ RHTPA application admin username. By default `admin`
. _TRUSTED_ADMIN_PASSWORD:_ RHTPA application admin password. By default `admin123456`
. _INIT_DATA:_ Location of keycloak <<_keycloak_clients,clients>> json files

[[_frontend_redirect_uris]]
===== Configure Frontend Redirect URIs

 * Navigate Keycloak admin console. Go to the **Clients** section and select the `frontend` client imported in <<_client_import,Client Import>>.
 * Add the application URL to the **Valid Redirect URIs** field, Which is `https://server{appDomain}`. For example, `https://server-tpa.apps.ocp.cluster.net`
 
[NOTE]
 Failure to update this field will result in a redirect URI error during application login.

==== Usage

For the RHTPA installation, the following OIDC values are retrieved from your Keycloak configuration:

* **issuerURL**: `_keycloakURL_/realms/_<Realm name>_`
* **cli**: Retrieve the **Client Secret** from the Keycloak admin console by navigating to **Clients** -> **cli** -> **Credentials** tab.
* **frontend**: `empty object as {}`
[NOTE]
This means no secret or specific configuration is needed for the OIDC setup to install RHTPA.

[[rhodf_operator_installation]]
=== Red Hat OpenShift Data Foundation Operator Configuration

This guide details the steps to configure and verify the Red Hat OpenShift Data Foundation Operator.

[[_prerequisites]]
==== Prerequisites

Before proceeding with the Openshift Data Foundation installation, ensure you have the following:

* Install the latest version of the link:https://github.com/noobaa/noobaa.github.io/blob/master/noobaa-operator-cli.md[NooBaa CLI].

.Optional: Add additional OpenShift Data Foundation nodes.

To avoid performance issues, add additional nodes to the Openshift Cluster:

 * To create additional nodes, run the following command:
+
[source, bash]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/create_machinesets.sh | bash
----
 * Wait for the new nodes to be in a `READY` and `AVAILABLE` state. Verify the Machineset status with:
+
[source, bash]
----
watch "oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'"
----
 * Confirm the nodes are ready for use:
+
[source, bash]
----
oc get nodes -l cluster.ocs.openshift.io/openshift-storage=
----

==== Installation

Follow these steps to install and configure the Openshift Data Foundation Operator:

* Create a dedicated namespace for the Openshift Data Foundation installation.
+
[source, bash]
----
oc create namespace openshift-storage
----
* Label the namespace to enable cluster monitoring.
+
[source, bash]
----
oc label namespace openshift-storage "openshift.io/cluster-monitoring=true"
----
* Install the Openshift Data Foundation Operator by following the official deployment link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html-single/deploying_openshift_data_foundation_on_any_platform/index#deploy-standalone-multicloud-object-gateway[guide].

* Confirm the Openshift Data Foundation installation is successful and the `StorageCluster` is in a `READY` state.
+
[source, bash]
----
oc get storagecluster -n openshift-storage ocs-storagecluster -o jsonpath='{.status.phase}{"\n"}'
----

[[_object_storage]]
==== Object Storage Configuration
After Openshift Data Foundation is installed, proceed with the following steps to configure and test object storage:

* Get the `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `External DNS` (under the S3 address section) from the `NooBaa` status command.
+
[source, bash]
----
noobaa status -n openshift-storage
----
* Create new object buckets by following the link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html-single/managing_hybrid_and_multicloud_resources/index#creating-new-buckets-using-mcg-object-browser_rhodf[guide].

[[_ocp_tls_certs]]
==== Export Default Openshift TLS Certs
* Export the cluster's default TLS certificate. This is often required for S3 clients to trust the endpoint.
+
[source, bash]
----
oc get secret -n openshift-ingress  router-certs-default -o go-template='{{index .data "tls.crt"}}' | base64 -d > tls.crt
----

[[_verify_s3]]
==== Verify S3 Connection
* Verify the connection to your S3 endpoint by exporting the Openshift Data Foundation storage <<_object_storage, details>> and SSL <<_ocp_tls_certs, certs>> using the AWS CLI.
+
[source, bash]
----
export AWS_ACCESS_KEY_ID=<AWS Access key>
export AWS_SECRET_ACCESS_KEY=<AWS Secret>
export AWS_CA_BUNDLE=<path to tls.crt>
aws  s3 ls --endpoint <External DNS>
----

.Optional: Add Bucket Policies
To add bucket policies to object buckets by using the Amazon Web Services (AWS) command-line interface to manage access permissions.

[[_bucket_policy]]
==== Create Bucket Policy
 * Create a `policy.json` file with the desired content. The example below grants `Allow` access to anyone; update the `Principal` section to restrict user permissions.
+
[source, json]
----
{
"Statement": [
      {
        "Effect": "Allow",
        "Principal": "*",
        "Action": [
            "s3:GetObject",
            "s3:DeleteObject",
            "s3:ListBucket",
            "s3:PutObject",
            "s3:ListAllMyBuckets"
        ],
        "Resource": ["arn:aws:s3:::<bucketname>","arn:aws:s3:::<bucketname>/*"]
      }
  ]
}
----
 * Run the following command to update the bucket policy:
+
[source, bash]
----
aws --endpoint <Noobaa External DNS Endpoint> s3api put-bucket-policy --bucket <bucket name> --policy file://<policy.json file path>
----

==== Usage
For the RHTPA installation, the following S3 values are retrieved from your Red Hat OpenShift Data Foundation installation:

* **type**: `s3`
* **region**: The external DNS from the noobaa status command. For more information, refer to <<_retrieve_noobaa_credentials_and_endpoint,Object Storage Configuration and Testing>> section.
* **bucket**: S3 Bucket created
* **accessKey**: AWS_ACCESS_KEY_ID
* **secretKey**: AWS_SECRET_ACCESS_KEY

Additionally, you must refer to the `tls.crt` file for installing with the `Values.tls.additionalTrustAnchor` option.

=== Red Hat Build of Opentelemetry for tracing and metrics
The Red Hat Build of Opentelemetry provides a way to collect and export telemetry data from your applications running in OpenShift. This guide outlines the steps to set up and configure the Opentelemetry Collector in your OpenShift environment.

[[_prerequisites_otel]]
==== Prerequisites

* Make sure you have cluster admin privileges to install the Opentelemetry Collector operator.
* Before installing - Add one or more tenants, and configure <<_tempo_tenant_configuration, read and write>> access. You can configure such an authorization setup by using a cluster role and cluster role binding for the Kubernetes Role-Based Access Control (RBAC). By default, no users are granted read or write permissions.

[[_otel_collector_installation]]
==== Installation

NOTE: Official documentation for Red Hat Build of OpenTelemetry Operator link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/red_hat_build_of_opentelemetry/index#install-otel[installation]

* Login to your OpenShift cluster as a cluster administrator.
* Go Operators -> OperatorHub in the OpenShift web console.
* Search for `Red Hat Build of Opentelemetry` and select the Red Hat Build of Opentelemetry operator.
* On the model window, Click on `Install`
* On the `Install Operator` page, make sure to align the following options:
  ** **Update Channel**: `stable`
  ** **Installation Mode**: `All namespaces on the cluster (default)`
  ** **Installed Namespace**: `Operator recommended namespace: openshift-opentelemetry-collector`
  ** **Update approval**: `Automatic`
* Click on `Install` to proceed with the installation.

[[_otel_collector_configuration]]
==== Configuration
* After the installation is complete, Go to the `Installed Operators` page.
* Select the RHTPA installation namespace, example `trustify`
* Click on the `Red Hat Build of OpenTelemetry`, Select `Opentelemetry Collector` and click on `Create Opentelemetry Collector` on the Operator details page.
* The OpenTelemetry requires a configuration file to define the collection and export settings for telemetry data. The below configuration is an example of the Opentelemetry Collector which collects data from RHTPA and exports it to Prometheus and Tempo.
+
[source, yaml]
----
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  # (1) Name of the OTEL collector instance
  name: dev
  # (2) namespace where the OTEL collector is deployed
  namespace: <Namespace>
spec:
  mode: deployment
  serviceAccount: otel-collector
  config:
    connectors:
      spanmetrics:
        metrics_flush_interval: 15s
    receivers:
      otlp:
        protocols:
          grpc:
          http:
      jaeger:
        protocols:
          thrift_binary:
          thrift_compact:
          thrift_http:
          grpc:
    extensions:
      bearertokenauth:
        filename: "/var/run/secrets/kubernetes.io/serviceaccount/token"
    processors: {}
    exporters:
      prometheus:
        endpoint: '0.0.0.0:8889'
        resource_to_telemetry_conversion:
          enabled: true
      otlp:
      # (3) Tempo traces endpoint
        endpoint: <Tempo traces endpoint (via OTLP)>
        tls:
          insecure: false
          ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt"
        auth:
          authenticator: bearertokenauth
        headers:
          X-Scope-OrgID: "dev"
    service:
      extensions: [bearertokenauth]
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          exporters: [otlp, spanmetrics]
        metrics:
          receivers: [otlp, spanmetrics]
          exporters: [prometheus]
      telemetry:
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: 0.0.0.0
                    port: 8888
----

**References:**

. _Name of the OpenTelemetry Collector instance:_ To identify the collector instance in the OpenShift cluster
. _Namespace where the OpenTelemetry Collector is deployed:_ To keep the permissions and access control simple, deploy OpenTelemetry Collector in the same namespace as the RHTPA installation. example `trustify`.
. _Tempo traces endpoint:_ The endpoint where the OpenTelemetry Collector sends the traces data. Traces can be configured within Openshift using Tempo operator (Refer <<_tempo_installation,Setting up Distributed Tracing for Tempo collector>> section). Example, `tempo-simplest-gateway.<namespace>.svc.cluster.local:8090`

[[_otel_collector_usage]]
==== Usage
* Verify the Opentelemetry collector with verification <<_verify_otel_collector, step>>
* Create <<_enable_monitoring,ServiceMonitors>> to capture metrics on Openshift web console under Observe -> Metrics.
* OTEL collector uses port `4317` for gRPC protocol to receive data from clients. Since all the serices and configurations are aligned in the same namespace, use the collector endpoint `dev-collector:4317`.
* To enable monitoring and tracing for the RHTPA installation use the below helm command,
+
[source, bash]
----
helm upgrade --install -n $NAMESPACE trustify openshift-helm-charts/redhat-trusted-profile-analyzer  --values PATH_TO_VALUES_FILE --set-string appDomain=$APP_DOMAIN --set tracing.enabled=true --set metrics.enabled=true --set-string collector.endpoint="grpc://dev-collector:4317"
----

[[_verify_otel_collector]]
==== Verify OpenTelemetry Collector
* Create the below job pointing to the otel collector endpoint to verify the tracing and metrics collection.
+
[source, yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: telemetrygen
  namespace: <TPA Namespace>
  labels:
    app: telmeetrygen
spec:
  ttlSecondsAfterFinished: 30
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: telemetrygen
        image: ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:v0.74.0
        args: [traces, --otlp-endpoint=dev-collector:4317, --otlp-insecure, --duration=240s, --rate=4]
----
* Go to Observe -> Traces in the OpenShift web console.
* Select `<namespace>/simplest` from the `Tempo instance` dropdown and select `telemetrygen` from the `Filter by Service Name` dropdown.
* The traces from the above job is captured and displayed in the Traces UI. The `--otlp-endpoint` points to the OpenTelemetry Collector service endpoint, Example `dev-collector:4317` referred for the `collector.endpoint` in the <<_otel_collector_usage, helm command>>.

[[_tempo_tenant_configuration]]
==== Tempo Tenant Configuration
It is mandate to define and configure one or more tenants and their read and write access.

NOTE: Official documentation for link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/distributed_tracing/index#configuring-permissions-and-tenants_distr-tracing-tempo-installing[Configuring permissions and tenants]

===== Configuring read permissions for tenants
* To add the tenants to a cluster role with read permissions to read traces:
+
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tempostack-traces-reader
rules:
  - apiGroups:
      - 'tempo.grafana.com'
    resources:
      - dev
    resourceNames:
      - traces
    verbs:
      - 'get'
----
* To grant authenticated users the read permissions for trace data, you can create a cluster role binding for the above cluster role with the following,
+
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tempostack-traces-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tempostack-traces-reader
subjects:
  - kind: Group
    apiGroup: rbac.authorization.k8s.io
    name: system:authenticated
----

===== Configuring write permissions for tenants

* Create a Service Account for the OpenTelemetry Collector
+
[source, yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  # (1) Namespace for the Service Account
  namespace: <Namespace>
----
**References:**

. _Namespace for the Service Account:_ To keep the permissions and access control simple, the Service Account is deployed in the same namespace as the RHTPA installation. example `trustify`.

* Add the tenants to a cluster role with the write (create) permissions to write traces.
+
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tempostack-traces-write
rules:
  - apiGroups:
      - 'tempo.grafana.com'
    resources:
      - dev
    resourceNames:
      - traces
    verbs:
      - 'create'
----
* Grant the OpenTelemetry Collector the write permissions by defining a cluster role binding to attach the OpenTelemetry Collector service account
+
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tempostack-traces
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tempostack-traces-write

subjects:
  - kind: ServiceAccount
    name: otel-collector
    # (1) Namespace for the Service Account
    namespace: <Namespace>
----
+
**References:**

. _Namespace for the Service Account:_ Specify the service account namespace. example `trustify`.

* <<_otel_collector_installation,Configure OpenTelemetry collector>> custom resource with the tenant information.

[[_tempo_installation]]
==== Setting up Distributed Tracing for Tempo collector:

[[_prerequisites_tempo]]
==== Prerequisites

* Tempo requires Object storage to store traces data. You can use the  <<rhodf_operator_installation, Red Hat OpenShift Data Foundation>> to set up object storage, or you can use any other S3 compatible storage service like AWS S3, <<_minio_installation, minio>>, etc.


===== Installation
NOTE: Official documentation for link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/red_hat_build_of_opentelemetry/index#install-otel[Distributed tracing]

* Go to Operators -> OperatorHub in the OpenShift web console.
* Search for "Tempo" and select `Tempo Operator provided by Red Hat`
* On the model window, Click on `Install`
* On the `Install Operator` page, make sure to align the following options:
  ** **Update Channel**: `stable`
  ** **Installation Mode**: `All namespaces on the cluster (default)`
  ** **Installed Namespace**: `Operator recommended namespace: openshift-tempo-operator`
  ** **Update approval**: `Automatic`
* Click on `Install` to proceed with the installation.

===== Configuration
* Create a secret with S3 storage credentials. Refer the official documentation for setting up link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/distributed_tracing/index#distr-tracing-tempo-object-storage-setup_distr-tracing-tempo-installing[object storage setup]
** Sample code block for S3 credentials secret given below:
+
[source, yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: s3-secret
  namespace: <Namespace>
stringData:
  bucket: <bucket_name>
  endpoint: <storage_endpoint>
  access_key_id: <access_key_id>
  access_key_secret: <access_key_secret>
type: Opaque
----

* After the installation is complete, Go to the `Installed Operators` page.
* Select the RHTPA installation namespace, example `trustify`
* Click on the `Tempo Operator`, Select `TempoStack` and click on `Create TempoStack` on the Operator details page.
* The TempoStack requires a configuration file to define the storage and tracing settings for telemetry data. The below configuration is an example of the TempoStack,
+
[source, yaml]
----
apiVersion: tempo.grafana.com/v1alpha1
kind: TempoStack
metadata:
  name: simplest
  # (1) Namespace for the TempoStack
  namespace: <Namespace>
spec:
  storage:
    secret:
    # (2) Secret containing the S3 credentials
      name: s3-secret
      type: s3
  storageSize: 1Gi
  resources:
    total:
      limits:
        memory: 2Gi
        cpu: 2000m
  tenants:
    mode: openshift
    authentication:
      - tenantName: dev
      # (3) Unique UUID for the tenant
        tenantId: "1610b0c3-c509-4592-a256-a1871353dbfa"
      - tenantName: prod
        tenantId: "1610b0c3-c509-4592-a256-a1871353dbfb"
  template:
    gateway:
      enabled: true
    queryFrontend:
      jaegerQuery:
        enabled: true
----

**References:**

. _Namespace for the TempoStack:_ To keep the permissions and access control simple, TempoStack is deployed in the same namespace as the RHTPA installation. example `trustify`.
. _Secret containing the S3 credentials:_ The secret containing the S3 credentials for the object storage.
. _Unique UUID for the tenant:_ The unique UUID for the tenant is used to identify the tenant in the TempoStack. It is recommended to use a unique UUID for each tenant.

====== Distributed tracing UI Plugin:
* Go to Operators -> OperatorHub in the OpenShift web console.
* Search for `Cluster Observability Operator` and select `Cluster Observability Operator`
* On the model window, Click on `Install`
* Go to Operator -> Installed Operator
* Select `Cluster Observability Operator`, Select `UIPlugin` and Click `Create UIPlugin` on the operator details page. Use the below configuration to enable the Traces UI plugin.
+
[source, yaml]
----
apiVersion: observability.openshift.io/v1alpha1
kind: UIPlugin
metadata:
  name: distributed-tracing
spec:
  type: DistributedTracing
----
* After the installation, Refresh the web console
* `Traces` option will be available in the left navigation menu of the OpenShift web console under `Observe` section
* <<_otel_collector_configuration, Configure OTEL collector>> with the tempo tracing endpoint for the otlp.endpoint under the exporters section.

[[_minio_installation]]
==== MinIO Installation
Use the following steps to install MinIO on your OpenShift cluster to provide S3 compatible storage for TempoStack

[source, yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
spec:
  selector:
    matchLabels:
      app: minio
  serviceName: "minio"
  replicas: 1
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
        - name: minio
          image: quay.io/minio/minio:latest
          args:
            - server
            - /data
          env:
            - name: MINIO_ROOT_USER
              value: "minioadmin"
            - name: MINIO_ROOT_PASSWORD
              value: "minioadmin123"
          ports:
            - containerPort: 9000
          volumeMounts:
            - name: storage
              mountPath: /data
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi
----
Create route for the minio service
[source, yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: minio
spec:
  selector:
    app: minio
  ports:
    - protocol: TCP
      port: 9000
      targetPort: 9000
----
Create a S3 object storage bucket in MinIO
[source, bash]
----
export AWS_ACCESS_KEY_ID=minioadmin
export AWS_SECRET_ACCESS_KEY=minioadmin123
aws s3 mb s3://<bucketname> --endpoint-url <minio_endpoint>
----

[[_enable_monitoring]]
==== Enable Monitoring for User defined Projects
NOTE: Official documentation for link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/configuring-user-workload-monitoring#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm[Enabling Monitoring for User defined Projects]

* Create or Edit ConfigMap to enable User workload monitoring
+
[source, yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
----
* Enabling User workload monitoring add prometheus operator under `openshift-user-workload-monitoring` namespace.
+
[source, bash]
----
oc get prometheus -n openshift-user-workload-monitoring
----
* Create ServiceMonitor for the OTEL collector Services targeting the prometheus port and metrics port
+
[source, yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector-app-metrics
  # (1) Namespace for the ServiceMonitor
  namespace: <Namespace>
  labels:
    openshift.io/user-monitoring: "true"
    release: user-workload
spec:
  selector:
    matchLabels:
    # (2) Match labels for the OTEL collector service
      app.kubernetes.io/name: dev-collector
  endpoints:
  - port: prometheus
    interval: 30s
    path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector-self-metrics
  # (1) Namespace for the ServiceMonitor
  namespace: <Namespace>
  labels:
    openshift.io/user-monitoring: "true"
    release: user-workload
spec:
  selector:
    matchLabels:
    # (2) Match labels for the OTEL collector service
      app.kubernetes.io/name: dev-collector-monitoring
  endpoints:
  - port: metrics
    interval: 30s
----
+
**References:**

. _Namespace for the ServiceMonitor:_ To keep the permissions and access control simple, the ServiceMonitor is deployed in the same namespace as the RHTPA installation. example `trustify`
. _Match labels for the OTEL collector service:_ The labels used to match the OTEL collector service. The labels should match the labels used in the OTEL collector service.

* After ServiceMonitor creation, Go to Observe -> Metrics in the OpenShift web console.
* With the `collector.endpoint` pointing to the OTEL collector with <<_otel_collector_usage,helm installation>>, The metrics from the OTEL collector service displayed on the Metrics graph for RHTPA. Enter the expression `http_server_duration_seconds_bucket` in the query field and click on `Run Query` to display the metrics.
