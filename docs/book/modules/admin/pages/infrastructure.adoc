
:sectlinks:

= Infrastructure

Trustify requires some infrastructure services for its installation. The services required are:

* OIDC provider
* PostgreSQL database instance
* Storage, either:
** Filesystem
** S3 compatible

Those services have to be provided by the user before the installation is being performed. Some information, like access
credentials, must be provided during the installation of Trustify.

There are different ways to set up such services. However, it is up to the user to provide those services and set them
up.

The following sections provide a few examples on how they can be provided in different ways. Keep in mind, those are just
examples, and you can modify them to suit your needs, or choose a different approach in providing those services.

== Self-managed Kubernetes

A simple approach is to use Keycloak as an OIDC provider, a PostgreSQL container, and a persistent volume claim for
the filesystem storage.

To set this up, it is possible to just use existing Helm charts for Keycloak and PostgreSQL. We do provide an
opinionated infrastructure Helm chart for this case at: https://github.com/trustification/trustify-helm-charts/tree/main/charts/trustify-infrastructure

You can install this using:

[source,bash]
----
NAMESPACE=trustify
APP_DOMAIN=public.cluster.domain
helm upgrade --install -n $NAMESPACE --repo https://trustification.io/trustify-helm-charts/ infrastructure trustify-infrastructure --values <values-file> --set-string keycloak.ingress.hostname=sso$APP_DOMAIN --set-string appDomain=$APP_DOMAIN
----

For this, you will need to provide a Helm "values" file. Which is a YAML file, providing additional information for
the Helm chart.

An example file, for Minikube, is:

[source,yaml]
----
keycloak:
  enabled: true
  production: false
  auth:
    adminUser: admin
    adminPassword: admin123456 # notsecret, replace
  tls:
    enabled: false
  service: {}
  ingress:
    enabled: true
    servicePort: http

oidc:
  clients:
    frontend: {}
    cli:
      clientSecret:
        value: 5460cc91-4e20-4edd-881c-b15b169f8a79 # notsecret, replace
----

== AWS services

It also is possible to use AWS managed services. The following AWS services can be used:

* OIDC provider: AWS Cognito
* PostgreSQL database instance: AWS RDS
* Storage: AWS S3

=== Manual setup

You can create the AWS resources manually, either through the AWS console or using the AWS CLI.

=== Terraform with OpenShift

Trustify also provides an example Terraform setup, which is intended to quickly deploy an opinionated set of services.
The Terraform scripts will create the AWS resources, as well as create Kubernetes resources with information from the
Terraform creation process, so that the Helm charts can pick up this information.

==== Main module

To use the Terraform scripts, you will need to create a wrapper/main module, referencing this `trustify` module.

NOTE: The following example file needs to be adapted to your needs.
Example values have to be replaced with values that suit your deployment.

[source,hcl-terraform]
----
provider "aws" {
  region  = "<your region>"  # <1>
  profile = "<your aws cli profile>" # <2>
}

provider "kubernetes" {
  config_path    = "<path to kubeconfig>" # <3>
  config_context = "<name of the kubectl context>" # <4>
}

variable "app-domain" {
  type = string
}

module "trustify" {
  source = "./trustify" # <5>

  cluster-vpc-id = "<your cluster vpc>" # <6>
  availability-zone = "<your availability zone inside your region>" # <7>

  namespace = "trustify"

  admin-email = "<your e-mail address>" # <8>
  sso-domain = "<a free cognito console domain name>" # <9>
  console-url = "https://server${var.app-domain}"
}
----
<1> The AWS region you want to create the resources in
<2> The name of the AWS CLI profile you want to use
<3> The location to the "kubeconfig" file, required for accessing the Kubernetes cluster
<4> The name of the Kubernetes client context (in the `kubeconfig`) to use
<5> The location of the `trustify` Terraform module
<6> The VPC ID of the OpenShift cluster, used to allow access to the RDS database
<7> The availability zone the RDS instance should be created in. Must be valid for the AWS region.
<8> The e-mail of the admin user, which will get frontend access to Trustification
<9> An AWS Cognito domain prefix. It is globally unique and has to be still available.

==== Creating the resources

First, initialize the OpenTofu instance.
This will set up the required providers and does not yet create any resources:

[source,bash]
----
tofu init
----

The following commands require the environment variable `APP_DOMAIN` to be set.
You can do this using the following command:

[source,bash]
----
NAMESPACE=trustify
APP_DOMAIN=-$NAMESPACE.$(kubectl -n openshift-ingress-operator get ingresscontrollers.operator.openshift.io default -o jsonpath='{.status.domain}')
----

Then, check if the resources can be created. This does not yet create the resources:

[source,bash]
----
tofu plan --var app-domain=$APP_DOMAIN
----

This will show you the resources which will get created and check if the creation is expected to be successful.

If this worked fine, proceed with actually creating the resources:

[source,bash]
----
tofu apply --var app-domain=$APP_DOMAIN
----

This will also create some resources in the Kubernetes cluster, including the credentials to the AWS accounts
created for accessing the created AWS resources.

=== Running the Helm chart

Prepare a "values" files, named `values-ocp-aws.yaml`:

[source,yaml]
----
ingress:
  className: openshift-default

authenticator:
  type: cognito

storage:
  type: s3
  region:
    valueFrom:
      configMapKeyRef:
        name: aws-storage
        key: region
  bucket: trustify
  accessKey:
    valueFrom:
      secretKeyRef:
        name: storage-credentials
        key: aws_access_key_id
  secretKey:
    valueFrom:
      secretKeyRef:
        name: storage-credentials
        key: aws_secret_access_key

database:
  host:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.host
  port:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.port
  name:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.name
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.port

createDatabase:
  name:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.name
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.password

migrateDatabase:
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.password

modules:
  createDatabase:
    enabled: true
  migrateDatabase:
    enabled: true

oidc:
  issuerUrl:
    valueFrom:
      configMapKeyRef:
        name: aws-oidc
        key: issuer-url
  clients:
    frontend:
      clientId:
        valueFrom:
          secretKeyRef:
            name: oidc-frontend
            key: client-id
    cli:
      clientId:
        valueFrom:
          secretKeyRef:
            name: oidc-cli
            key: client-id
      clientSecret:
        valueFrom:
          secretKeyRef:
            name: oidc-cli
            key: client-secret
----

You can now run the Helm chart using the following command:

[source,bash]
----
helm upgrade --install --repo https://trustification.io/trustify-helm-charts/ --devel -n $NAMESPACE trustify charts/trustify --values values-ocp-aws.yaml --set-string appDomain=$APP_DOMAIN
----

NOTE: The `--devel` flag is currently necessary as the Helm chart has a pre-release version.

== Red Hat Services on Openshift

Install the following Red Hat services:

* Red Hat Single Sign-on (SSO) operator as the OpenID Connect (OIDC) provider.
* Red Hat OpenShift Data Foundation operator as the storage provider.

=== Manual setup

==== Red Hat Single Sign-on operator

 * Install Single Sign-on operator with deployment https://docs.redhat.com/en/documentation/red_hat_single_sign-on/7.6/html/server_installation_and_configuration_guide/operator#installing-operator[guide].
 * Navigate to the related Keycloak instance of RHSSO operator and login to the Admin console with valid credentials.

Complete the following steps to configure Keycloak:

[[_realm_creation]]
===== Realm Creation

 * Create a new **Realm** within your Keycloak instance.

[[_role_definition]]
===== Role Definition

 * Create a custom role, for example, `trust-admin`.
 * Assign the `trust-admin` role to the default roles for your newly created realm.
 * Navigate to **Realm Settings** -> **Roles** tab -> Select the `default-roles-{your-realm-name}` role -> **Role Mappings** tab -> **Assign Role** to add `trust-admin`.

[[_client_scope_definition]]
===== Client Scope Definition

 * Create the following **Client Scopes** with the `openid-connect` protocol:
  ** `read:document`
  ** `create:document`
  ** `update:document`
  ** `delete:document`

[[_assign_role_to_client_scope]]
===== Assign Roles to Client Scopes

 * After creating the Client Scopes in <<_client_scope_definition,Client Scope Definition>>, navigate to each individual **Client Scope**.
 * Select the **Scope** tab within each Client Scope's settings.
 * Move the `trust-admin` role (created in <<_role_definition,Role Definition>>) from the Available Roles to the Assigned Roles for each scope.

[[_client_import]]
===== Client Import

 * Go to Keycloak administration console -> Go to **Clients** section.
 * Click the **Create** button -> then **Import**.
 * Select and import the following client configuration files:
 ** `frontend.json`
+
[source, json]
----
{
    "clientId": "frontend",
    "clientAuthenticatorType": "client-secret",
    "enabled": true,
    "publicClient": true,
    "implicitFlowEnabled": true,
    "standardFlowEnabled": true,
    "directAccessGrantsEnabled": false,
    "serviceAccountsEnabled": false,
    "fullScopeAllowed": true,
    "webOrigins": [
      "*"
    ],
    "defaultClientScopes": [
      "basic",
      "email",
      "profile",
      "roles",
      "web-origins",
      "create:document",
      "read:document",
      "update:document",
      "delete:document"
    ],
    "optionalClientScopes": [
      "address",
      "microprofile-jwt",
      "offline_access",
      "phone"
    ],
    "attributes": {
      "access.token.lifespan": "300",
      "post.logout.redirect.uris": "+"
    }
  }
----
 ** `cli.json`
+
[source, json]
----
{
  "clientId": "cli",
  "clientAuthenticatorType": "client-secret",
  "enabled": true,
  "publicClient": false,
  "implicitFlowEnabled": false,
  "standardFlowEnabled": false,
  "directAccessGrantsEnabled": false,
  "serviceAccountsEnabled": true,
  "fullScopeAllowed": true,
  "defaultClientScopes": [
    "basic",
    "email",
    "profile",
    "roles",
    "web-origins",
    "create:document",
    "read:document",
    "update:document",
    "delete:document"
  ],
  "optionalClientScopes": [
    "address",
    "microprofile-jwt",
    "offline_access",
    "phone"
  ],
  "attributes": {
    "access.token.lifespan": "300",
    "post.logout.redirect.uris": "+"
  }
}
----

[[_user_management]]
===== User Management

 * Go to the **Users** section and add a new user.
 * Go to the **Role Mapping** tab for this user, and assign the `trust-admin` role to the user.
 * Under the **Credentials** tab, set a password for this user.

[[_cli_client_secret]]
===== Retrieve CLI Client Secret

 * Navigate to the **Clients** section and select the `cli` client that you imported in <<_client_import,Client Import>>.
 * Go to the **Credentials** tab.
 * Retrieve the **secret** displayed here. This secret is essential for the Helm chart installation.

[[_frontend_redirect_uris]]
===== Configure Frontend Redirect URIs

 * Navigate to the **Clients** section and select the `frontend` client that you imported in <<_client_import,Client Import>>.
 * In the **Valid Redirect URIs** field, add the application URL that will be used after the Helm installation which is `https://server{appDomain}`.
 
[NOTE]
 Failure to update this field will result in a redirect URI error during application login.

===== Usage

For the RHTPA installation, the following OIDC values are retrieved from your Keycloak (RH-SSO) configuration:

* **issuerURL**: `_keycloakURL_/realms/<<_realm_creation,Realm name>>`
* **frontend**: `empty object as {}`

[NOTE]
This means no secret or specific configuration is needed for the OIDC setup to install RHTPA.

* **cli**: Retrieve the **Client Secret** from the Keycloak admin console by navigating to **Clients** -> **cli** -> **Credentials** tab.

==== Red Hat OpenShift Data Foundation Operator Configuration

This guide details the steps to configure and verify the Red Hat OpenShift Data Foundation Operator.

[[_prerequisites]]
===== Prerequisites

Before proceeding with the Openshift Data Foundation installation, ensure you have the following:

* Install the latest version of the link:https://github.com/noobaa/noobaa.github.io/blob/master/noobaa-operator-cli.md[NooBaa CLI].

.Optional: Add additional OpenShift Data Foundation nodes.

To avoid performance issues, add additional nodes to the Openshift Cluster:

 * To create additional nodes, run the following command:
+
[source, bash]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/create_machinesets.sh | bash
----
 * Wait for the new nodes to be in a `READY` and `AVAILABLE` state. Verify the Machineset status with:
+
[source, bash]
----
watch "oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'"
----
 * Confirm the nodes are ready for use:
+
[source, bash]
----
oc get nodes -l cluster.ocs.openshift.io/openshift-storage=
----

===== Installation

Follow these steps to install and configure the Openshift Data Foundation Operator:

* Create a dedicated namespace for the Openshift Data Foundation installation.
+
[source, bash]
----
oc create namespace openshift-storage
----
* Label the namespace to enable cluster monitoring.
+
[source, bash]
----
oc label namespace openshift-storage "openshift.io/cluster-monitoring=true"
----
* Install the Openshift Data Foundation Operator by following the official deployment link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html-single/deploying_openshift_data_foundation_on_any_platform/index#deploy-standalone-multicloud-object-gateway[guide].

* Confirm the Openshift Data Foundation installation is successful and the `StorageCluster` is in a `READY` state.
+
[source, bash]
----
oc get storagecluster -n openshift-storage ocs-storagecluster -o jsonpath='{.status.phase}{"\n"}'
----

[[_object_storage]]
===== Object Storage Configuration
After Openshift Data Foundation is installed, proceed with the following steps to configure and test object storage:

* Get the `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `External DNS` (under the S3 address section) from the `NooBaa` status command.
+
[source, bash]
----
noobaa status -n openshift-storage
----
* Create new object buckets by following the link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html-single/managing_hybrid_and_multicloud_resources/index#creating-new-buckets-using-mcg-object-browser_rhodf[guide].

[[_ocp_tls_certs]]
===== Export Default Openshift TLS Certs
* Export the cluster's default TLS certificate. This is often required for S3 clients to trust the endpoint.
+
[source, bash]
----
oc get secret -n openshift-ingress  router-certs-default -o go-template='{{index .data "tls.crt"}}' | base64 -d > tls.crt
----

[[_verify_s3]]
===== Verify S3 Connection
* Verify the connection to your S3 endpoint by exporting the Openshift Data Foundation storage <<_object_storage, details>> and SSL <<_ocp_tls_certs, certs>> using the AWS CLI.
+
[source, bash]
----
export AWS_ACCESS_KEY_ID=<AWS Access key>
export AWS_SECRET_ACCESS_KEY=<AWS Secret>
export AWS_CA_BUNDLE=<path to tls.crt>
aws  s3 ls --endpoint <External DNS>
----

.Optional: Add Bucket Policies
To add bucket policies to object buckets by using the Amazon Web Services (AWS) command-line interface to manage access permissions.

[[_bucket_policy]]
===== Create Bucket Policy
 * Create a `policy.json` file with the desired content. The example below grants `Allow` access to anyone; update the `Principal` section to restrict user permissions.
+
[source, json]
----
{
"Statement": [
      {
        "Effect": "Allow",
        "Principal": "*",
        "Action": [
            "s3:GetObject",
            "s3:DeleteObject",
            "s3:ListBucket",
            "s3:PutObject",
            "s3:ListAllMyBuckets"
        ],
        "Resource": ["arn:aws:s3:::<bucketname>","arn:aws:s3:::<bucketname>/*"]
      }
  ]
}
----
 * Run the following command to update the bucket policy:
+
[source, bash]
----
aws --endpoint <Noobaa External DNS Endpoint> s3api put-bucket-policy --bucket <bucket name> --policy file://<policy.json file path>
----

===== Usage
For the RHTPA installation, the following S3 values are retrieved from your Red Hat OpenShift Data Foundation installation:

* **type**: `s3`
* **region**: The external DNS from the noobaa status command. For more information, refer to <<_retrieve_noobaa_credentials_and_endpoint,Object Storage Configuration and Testing>> section.
* **bucket**: S3 Bucket created
* **accessKey**: AWS_ACCESS_KEY_ID 
* **secretKey**: AWS_SECRET_ACCESS_KEY

Additionally, you must refer to the `tls.crt` file for installing with the `Values.tls.additionalTrustAnchor` option.

