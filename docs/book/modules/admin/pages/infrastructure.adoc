
:sectlinks:

= Infrastructure

Trustify requires some infrastructure services for its installation. The services required are:

* OIDC provider
* PostgreSQL database instance
* Storage, either:
** Filesystem
** S3 compatible

Those services have to be provided by the user before the installation is being performed. Some information, like access
credentials, must be provided during the installation of Trustify.

There are different ways to set up such services. However, it is up to the user to provide those services and set them
up.

The following sections provide a few examples on how they can be provided in different ways. Keep in mind, those are just
examples, and you can modify them to suit your needs, or choose a different approach in providing those services.

== Self-managed Kubernetes

A simple approach is to use Keycloak as an OIDC provider, a PostgreSQL container, and a persistent volume claim for
the filesystem storage.

To set this up, it is possible to just use existing Helm charts for Keycloak and PostgreSQL. We do provide an
opinionated infrastructure Helm chart for this case at: https://github.com/trustification/trustify-helm-charts/tree/main/charts/trustify-infrastructure

You can install this using:

[source,bash]
----
NAMESPACE=trustify
APP_DOMAIN=public.cluster.domain
helm upgrade --install -n $NAMESPACE --repo https://trustification.io/trustify-helm-charts/ infrastructure trustify-infrastructure --values <values-file> --set-string keycloak.ingress.hostname=sso$APP_DOMAIN --set-string appDomain=$APP_DOMAIN
----

For this, you will need to provide a Helm "values" file. Which is a YAML file, providing additional information for
the Helm chart.

An example file, for Minikube, is:

[source,yaml]
----
keycloak:
  enabled: true
  production: false
  auth:
    adminUser: admin
    adminPassword: admin123456 # notsecret, replace
  tls:
    enabled: false
  service: {}
  ingress:
    enabled: true
    servicePort: http

oidc:
  clients:
    frontend: {}
    cli:
      clientSecret:
        value: 5460cc91-4e20-4edd-881c-b15b169f8a79 # notsecret, replace
----

=== Collecting traces and metrics

To enable observability, you must configure application-level telemetry using
command-line flags. Both metrics and traces are optional and can be enabled independently
or together. However, an OpenTelemetry Collector endpoint is required when either signal is enabled.

As Trustify do not ship with an OpenTelemetry Collector, we do provide an opinionated
infrastructure Helm chart for this case at: https://github.com/trustification/trustify-helm-charts/tree/main/charts/trustify-infrastructure

Telemetry is configured via Helm values, which are rendered as command-line flags
passed to the application on startup. Example with Minikube:

[source,bash]
----
helm upgrade --install -n $NAMESPACE \
  --repo https://trustification.io/trustify-helm-charts/ trustify trustify \
  --values values-minikube.yaml \
  --set-string appDomain=$APP_DOMAIN \
  --set tracing.enabled=true \
  --set metrics.enabled=true \
  --set-string collector.endpoint="http://infrastructure-otelcol:4317"
----

Optionally, the following OpenTelemetry settings can be customized using Helm CLI flags:

[source,bash]
----
--set metrics.otelMetricExportInterval=1000
--set tracing.otelBspMaxExportBatchSize=64
--set tracing.otelTracesSampler="traceidratio"
--set tracing.otelTracesSamplerArg=1
----

It will override the following environment variables:

* The maximum number of spans per batch, controlled by `OTEL_BSP_MAX_EXPORT_BATCH_SIZE`.
* The trace sampling strategy, set via `OTEL_TRACES_SAMPLER`.
* The trace sampling probability, configured using `OTEL_TRACES_SAMPLER_ARG`.
* The metrics export interval, which defaults to 60 seconds and can be adjusted via `OTEL_METRIC_EXPORT_INTERVAL`.

For Minikube, we are using the following default values:

[source,yaml]
----
- name: OTEL_METRIC_EXPORT_INTERVAL
  value: "60000"
- name: OTEL_BSP_MAX_EXPORT_BATCH_SIZE
  value: "32"
- name: OTEL_TRACES_SAMPLER
  value: parentbased_traceidratio
- name: OTEL_TRACES_SAMPLER_ARG
  value: "0.1"
----

== AWS services

It also is possible to use AWS managed services. The following AWS services can be used:

* OIDC provider: AWS Cognito
* PostgreSQL database instance: AWS RDS
* Storage: AWS S3

=== Manual setup

You can create the AWS resources manually, either through the AWS console or using the AWS CLI.

=== Terraform with OpenShift

Trustify also provides an example Terraform setup, which is intended to quickly deploy an opinionated set of services.
The Terraform scripts will create the AWS resources, as well as create Kubernetes resources with information from the
Terraform creation process, so that the Helm charts can pick up this information.

==== Main module

To use the Terraform scripts, you will need to create a wrapper/main module, referencing this `trustify` module.

NOTE: The following example file needs to be adapted to your needs.
Example values have to be replaced with values that suit your deployment.

[source,hcl-terraform]
----
provider "aws" {
  region  = "<your region>"  # <1>
  profile = "<your aws cli profile>" # <2>
}

provider "kubernetes" {
  config_path    = "<path to kubeconfig>" # <3>
  config_context = "<name of the kubectl context>" # <4>
}

variable "app-domain" {
  type = string
}

module "trustify" {
  source = "./trustify" # <5>

  cluster-vpc-id = "<your cluster vpc>" # <6>
  availability-zone = "<your availability zone inside your region>" # <7>

  namespace = "trustify"

  admin-email = "<your e-mail address>" # <8>
  sso-domain = "<a free cognito console domain name>" # <9>
  console-url = "https://server${var.app-domain}"
}
----
<1> The AWS region you want to create the resources in
<2> The name of the AWS CLI profile you want to use
<3> The location to the "kubeconfig" file, required for accessing the Kubernetes cluster
<4> The name of the Kubernetes client context (in the `kubeconfig`) to use
<5> The location of the `trustify` Terraform module
<6> The VPC ID of the OpenShift cluster, used to allow access to the RDS database
<7> The availability zone the RDS instance should be created in. Must be valid for the AWS region.
<8> The e-mail of the admin user, which will get frontend access to Trustification
<9> An AWS Cognito domain prefix. It is globally unique and has to be still available.

==== Creating the resources

First, initialize the OpenTofu instance.
This will set up the required providers and does not yet create any resources:

[source,bash]
----
tofu init
----

The following commands require the environment variable `APP_DOMAIN` to be set.
You can do this using the following command:

[source,bash]
----
NAMESPACE=trustify
APP_DOMAIN=-$NAMESPACE.$(kubectl -n openshift-ingress-operator get ingresscontrollers.operator.openshift.io default -o jsonpath='{.status.domain}')
----

Then, check if the resources can be created. This does not yet create the resources:

[source,bash]
----
tofu plan --var app-domain=$APP_DOMAIN
----

This will show you the resources which will get created and check if the creation is expected to be successful.

If this worked fine, proceed with actually creating the resources:

[source,bash]
----
tofu apply --var app-domain=$APP_DOMAIN
----

This will also create some resources in the Kubernetes cluster, including the credentials to the AWS accounts
created for accessing the created AWS resources.

=== Running the Helm chart

Prepare a "values" files, named `values-ocp-aws.yaml`:

[source,yaml]
----
ingress:
  className: openshift-default

authenticator:
  type: cognito

storage:
  type: s3
  region:
    valueFrom:
      configMapKeyRef:
        name: aws-storage
        key: region
  bucket: trustify
  accessKey:
    valueFrom:
      secretKeyRef:
        name: storage-credentials
        key: aws_access_key_id
  secretKey:
    valueFrom:
      secretKeyRef:
        name: storage-credentials
        key: aws_secret_access_key

database:
  host:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.host
  port:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.port
  name:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.name
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-credentials
        key: db.port

createDatabase:
  name:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.name
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.password

migrateDatabase:
  username:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.user
  password:
    valueFrom:
      secretKeyRef:
        name: postgresql-admin-credentials
        key: db.password

modules:
  createDatabase:
    enabled: true
  migrateDatabase:
    enabled: true

oidc:
  issuerUrl:
    valueFrom:
      configMapKeyRef:
        name: aws-oidc
        key: issuer-url
  clients:
    frontend:
      clientId:
        valueFrom:
          secretKeyRef:
            name: oidc-frontend
            key: client-id
    cli:
      clientId:
        valueFrom:
          secretKeyRef:
            name: oidc-cli
            key: client-id
      clientSecret:
        valueFrom:
          secretKeyRef:
            name: oidc-cli
            key: client-secret
----

You can now run the Helm chart using the following command:

[source,bash]
----
helm upgrade --install --repo https://trustification.io/trustify-helm-charts/ --devel -n $NAMESPACE trustify charts/trustify --values values-ocp-aws.yaml --set-string appDomain=$APP_DOMAIN
----

NOTE: The `--devel` flag is currently necessary as the Helm chart has a pre-release version.

== Red Hat Services on Openshift

Install the following Red Hat services:

* Red Hat Single Sign-on (SSO) operator as the OpenID Connect (OIDC) provider.
* Red Hat OpenShift Data Foundation operator as the storage provider.
* Red Hat Build of Opentelemetry for tracing and metrics.

=== Manual setup

=== Red Hat Single Sign-on operator

 * Install Single Sign-on operator with deployment https://docs.redhat.com/en/documentation/red_hat_single_sign-on/7.6/html/server_installation_and_configuration_guide/operator#installing-operator[guide].
 * Navigate to the related Keycloak instance of RHSSO operator and login to the Admin console with valid credentials.

Complete the following steps to configure Keycloak:

[[_realm_creation]]
==== Realm Creation

 * Create a new **Realm** within your Keycloak instance.

[[_role_definition]]
==== Role Definition

 * Create a custom role, for example, `trust-admin`.
 * Assign the `trust-admin` role to the default roles for your newly created realm.
 * Navigate to **Realm Settings** -> **Roles** tab -> Select the `default-roles-{your-realm-name}` role -> **Role Mappings** tab -> **Assign Role** to add `trust-admin`.

[[_client_scope_definition]]
==== Client Scope Definition

 * Create the following **Client Scopes** with the `openid-connect` protocol:
  ** `read:document`
  ** `create:document`
  ** `update:document`
  ** `delete:document`

[[_assign_role_to_client_scope]]
==== Assign Roles to Client Scopes

 * After creating the Client Scopes in <<_client_scope_definition,Client Scope Definition>>, navigate to each individual **Client Scope**.
 * Select the **Scope** tab within each Client Scope's settings.
 * Move the `trust-admin` role (created in <<_role_definition,Role Definition>>) from the Available Roles to the Assigned Roles for each scope.

[[_client_import]]
==== Client Import

 * Go to Keycloak administration console -> Go to **Clients** section.
 * Click the **Create** button -> then **Import**.
 * Select and import the following client configuration files:
 ** `frontend.json`
+
[source, json]
----
{
    "clientId": "frontend",
    "clientAuthenticatorType": "client-secret",
    "enabled": true,
    "publicClient": true,
    "implicitFlowEnabled": true,
    "standardFlowEnabled": true,
    "directAccessGrantsEnabled": false,
    "serviceAccountsEnabled": false,
    "fullScopeAllowed": true,
    "webOrigins": [
      "*"
    ],
    "defaultClientScopes": [
      "basic",
      "email",
      "profile",
      "roles",
      "web-origins",
      "create:document",
      "read:document",
      "update:document",
      "delete:document"
    ],
    "optionalClientScopes": [
      "address",
      "microprofile-jwt",
      "offline_access",
      "phone"
    ],
    "attributes": {
      "access.token.lifespan": "300",
      "post.logout.redirect.uris": "+"
    }
  }
----
 ** `cli.json`
+
[source, json]
----
{
  "clientId": "cli",
  "clientAuthenticatorType": "client-secret",
  "enabled": true,
  "publicClient": false,
  "implicitFlowEnabled": false,
  "standardFlowEnabled": false,
  "directAccessGrantsEnabled": false,
  "serviceAccountsEnabled": true,
  "fullScopeAllowed": true,
  "defaultClientScopes": [
    "basic",
    "email",
    "profile",
    "roles",
    "web-origins",
    "create:document",
    "read:document",
    "update:document",
    "delete:document"
  ],
  "optionalClientScopes": [
    "address",
    "microprofile-jwt",
    "offline_access",
    "phone"
  ],
  "attributes": {
    "access.token.lifespan": "300",
    "post.logout.redirect.uris": "+"
  }
}
----

[[_user_management]]
==== User Management

 * Go to the **Users** section and add a new user.
 * Go to the **Role Mapping** tab for this user, and assign the `trust-admin` role to the user.
 * Under the **Credentials** tab, set a password for this user.

[[_cli_client_secret]]
==== Retrieve CLI Client Secret

 * Navigate to the **Clients** section and select the `cli` client that you imported in <<_client_import,Client Import>>.
 * Go to the **Credentials** tab.
 * Retrieve the **secret** displayed here. This secret is essential for the Helm chart installation.

[[_frontend_redirect_uris]]
==== Configure Frontend Redirect URIs

 * Navigate to the **Clients** section and select the `frontend` client that you imported in <<_client_import,Client Import>>.
 * In the **Valid Redirect URIs** field, add the application URL that will be used after the Helm installation which is `https://server{appDomain}`.

[NOTE]
 Failure to update this field will result in a redirect URI error during application login.

==== Usage

For the RHTPA installation, the following OIDC values are retrieved from your Keycloak (RH-SSO) configuration:

* **issuerURL**: `_keycloakURL_/realms/<<_realm_creation,Realm name>>`
* **frontend**: `empty object as {}`

[NOTE]
This means no secret or specific configuration is needed for the OIDC setup to install RHTPA.

* **cli**: Retrieve the **Client Secret** from the Keycloak admin console by navigating to **Clients** -> **cli** -> **Credentials** tab.

[[rhodf_operator_installation]]
=== Red Hat OpenShift Data Foundation Operator Configuration

This guide details the steps to configure and verify the Red Hat OpenShift Data Foundation Operator.

[[_prerequisites]]
==== Prerequisites

Before proceeding with the Openshift Data Foundation installation, ensure you have the following:

* Install the latest version of the link:https://github.com/noobaa/noobaa.github.io/blob/master/noobaa-operator-cli.md[NooBaa CLI].

.Optional: Add additional OpenShift Data Foundation nodes.

To avoid performance issues, add additional nodes to the Openshift Cluster:

 * To create additional nodes, run the following command:
+
[source, bash]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/create_machinesets.sh | bash
----
 * Wait for the new nodes to be in a `READY` and `AVAILABLE` state. Verify the Machineset status with:
+
[source, bash]
----
watch "oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'"
----
 * Confirm the nodes are ready for use:
+
[source, bash]
----
oc get nodes -l cluster.ocs.openshift.io/openshift-storage=
----

==== Installation

Follow these steps to install and configure the Openshift Data Foundation Operator:

* Create a dedicated namespace for the Openshift Data Foundation installation.
+
[source, bash]
----
oc create namespace openshift-storage
----
* Label the namespace to enable cluster monitoring.
+
[source, bash]
----
oc label namespace openshift-storage "openshift.io/cluster-monitoring=true"
----
* Install the Openshift Data Foundation Operator by following the official deployment link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html-single/deploying_openshift_data_foundation_on_any_platform/index#deploy-standalone-multicloud-object-gateway[guide].

* Confirm the Openshift Data Foundation installation is successful and the `StorageCluster` is in a `READY` state.
+
[source, bash]
----
oc get storagecluster -n openshift-storage ocs-storagecluster -o jsonpath='{.status.phase}{"\n"}'
----

[[_object_storage]]
==== Object Storage Configuration
After Openshift Data Foundation is installed, proceed with the following steps to configure and test object storage:

* Get the `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `External DNS` (under the S3 address section) from the `NooBaa` status command.
+
[source, bash]
----
noobaa status -n openshift-storage
----
* Create new object buckets by following the link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html-single/managing_hybrid_and_multicloud_resources/index#creating-new-buckets-using-mcg-object-browser_rhodf[guide].

[[_ocp_tls_certs]]
==== Export Default Openshift TLS Certs
* Export the cluster's default TLS certificate. This is often required for S3 clients to trust the endpoint.
+
[source, bash]
----
oc get secret -n openshift-ingress  router-certs-default -o go-template='{{index .data "tls.crt"}}' | base64 -d > tls.crt
----

[[_verify_s3]]
==== Verify S3 Connection
* Verify the connection to your S3 endpoint by exporting the Openshift Data Foundation storage <<_object_storage, details>> and SSL <<_ocp_tls_certs, certs>> using the AWS CLI.
+
[source, bash]
----
export AWS_ACCESS_KEY_ID=<AWS Access key>
export AWS_SECRET_ACCESS_KEY=<AWS Secret>
export AWS_CA_BUNDLE=<path to tls.crt>
aws  s3 ls --endpoint <External DNS>
----

.Optional: Add Bucket Policies
To add bucket policies to object buckets by using the Amazon Web Services (AWS) command-line interface to manage access permissions.

[[_bucket_policy]]
==== Create Bucket Policy
 * Create a `policy.json` file with the desired content. The example below grants `Allow` access to anyone; update the `Principal` section to restrict user permissions.
+
[source, json]
----
{
"Statement": [
      {
        "Effect": "Allow",
        "Principal": "*",
        "Action": [
            "s3:GetObject",
            "s3:DeleteObject",
            "s3:ListBucket",
            "s3:PutObject",
            "s3:ListAllMyBuckets"
        ],
        "Resource": ["arn:aws:s3:::<bucketname>","arn:aws:s3:::<bucketname>/*"]
      }
  ]
}
----
 * Run the following command to update the bucket policy:
+
[source, bash]
----
aws --endpoint <Noobaa External DNS Endpoint> s3api put-bucket-policy --bucket <bucket name> --policy file://<policy.json file path>
----

==== Usage
For the RHTPA installation, the following S3 values are retrieved from your Red Hat OpenShift Data Foundation installation:

* **type**: `s3`
* **region**: The external DNS from the noobaa status command. For more information, refer to <<_retrieve_noobaa_credentials_and_endpoint,Object Storage Configuration and Testing>> section.
* **bucket**: S3 Bucket created
* **accessKey**: AWS_ACCESS_KEY_ID
* **secretKey**: AWS_SECRET_ACCESS_KEY

Additionally, you must refer to the `tls.crt` file for installing with the `Values.tls.additionalTrustAnchor` option.

=== Red Hat Build of Opentelemetry for tracing and metrics
The Red Hat Build of Opentelemetry provides a way to collect and export telemetry data from your applications running in OpenShift. This guide outlines the steps to set up and configure the Opentelemetry Collector in your OpenShift environment.

[[_prerequisites_otel]]
==== Prerequisites

* Make sure you have cluster admin privileges to install the Opentelemetry Collector operator.
* Before installing - Add one or more tenants, and configure <<_tempo_tenant_configuration, read and write>> access. You can configure such an authorization setup by using a cluster role and cluster role binding for the Kubernetes Role-Based Access Control (RBAC). By default, no users are granted read or write permissions.

[[_otel_collector_installation]]
==== Installation

NOTE: Official documentation for Red Hat Build of OpenTelemetry Operator link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/red_hat_build_of_opentelemetry/index#install-otel[installation]

* Login to your OpenShift cluster as a cluster administrator.
* Go Operators -> OperatorHub in the OpenShift web console.
* Search for `Red Hat Build of Opentelemetry` and select the Red Hat Build of Opentelemetry operator.
* On the model window, Click on `Install`
* On the `Install Operator` page, make sure to align the following options:
  ** **Update Channel**: `stable`
  ** **Installation Mode**: `All namespaces on the cluster (default)`
  ** **Installed Namespace**: `Operator recommended namespace: openshift-opentelemetry-collector`
  ** **Update approval**: `Automatic`
* Click on `Install` to proceed with the installation.

[[_otel_collector_configuration]]
==== Configuration
* After the installation is complete, Go to the `Installed Operators` page.
* Select the RHTPA installation namespace, example `trustify`
* Click on the `Red Hat Build of OpenTelemetry`, Select `Opentelemetry Collector` and click on `Create Opentelemetry Collector` on the Operator details page.
* The OpenTelemetry requires a configuration file to define the collection and export settings for telemetry data. The below configuration is an example of the Opentelemetry Collector which collects data from RHTPA and exports it to Prometheus and Tempo.
+
[source, yaml]
----
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  # (1) Name of the OTEL collector instance
  name: dev
  # (2) namespace where the OTEL collector is deployed
  namespace: <Namespace>
spec:
  mode: deployment
  serviceAccount: otel-collector
  config:
    connectors:
      spanmetrics:
        metrics_flush_interval: 15s
    receivers:
      otlp:
        protocols:
          grpc:
          http:
      jaeger:
        protocols:
          thrift_binary:
          thrift_compact:
          thrift_http:
          grpc:
    extensions:
      bearertokenauth:
        filename: "/var/run/secrets/kubernetes.io/serviceaccount/token"
    processors: {}
    exporters:
      prometheus:
        endpoint: '0.0.0.0:8889'
        resource_to_telemetry_conversion:
          enabled: true
      otlp:
      # (3) Tempo traces endpoint
        endpoint: <Tempo traces endpoint (via OTLP)>
        tls:
          insecure: false
          ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt"
        auth:
          authenticator: bearertokenauth
        headers:
          X-Scope-OrgID: "dev"
    service:
      extensions: [bearertokenauth]
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          exporters: [otlp, spanmetrics]
        metrics:
          receivers: [otlp, spanmetrics]
          exporters: [prometheus]
      telemetry:
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: 0.0.0.0
                    port: 8888
----

**References:**

. _Name of the OpenTelemetry Collector instance:_ To identify the collector instance in the OpenShift cluster
. _Namespace where the OpenTelemetry Collector is deployed:_ To keep the permissions and access control simple, deploy OpenTelemetry Collector in the same namespace as the RHTPA installation. example `trustify`.
. _Tempo traces endpoint:_ The endpoint where the OpenTelemetry Collector sends the traces data. Traces can be configured within Openshift using Tempo operator (Refer <<_tempo_installation,Setting up Distributed Tracing for Tempo collector>> section). Example, `tempo-simplest-gateway.<namespace>.svc.cluster.local:8090`

[[_otel_collector_usage]]
==== Usage
* Verify the Opentelemetry collector with verification <<_verify_otel_collector, step>>
* Create <<_enable_monitoring,ServiceMonitors>> to capture metrics on Openshift web console under Observe -> Metrics.
* OTEL collector uses port `4317` for gRPC protocol to receive data from clients. Since all the serices and configurations are aligned in the same namespace, use the collector endpoint `dev-collector:4317`.
* To enable monitoring and tracing for the RHTPA installation use the below helm command,
+
[source, bash]
----
helm upgrade --install -n $NAMESPACE trustify openshift-helm-charts/redhat-trusted-profile-analyzer  --values PATH_TO_VALUES_FILE --set-string appDomain=$APP_DOMAIN --set tracing.enabled=true --set metrics.enabled=true --set-string collector.endpoint="grpc://dev-collector:4317"
----

[[_verify_otel_collector]]
==== Verify OpenTelemetry Collector
* Create the below job pointing to the otel collector endpoint to verify the tracing and metrics collection.
+
[source, yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: telemetrygen
  namespace: <TPA Namespace>
  labels:
    app: telmeetrygen
spec:
  ttlSecondsAfterFinished: 30
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: telemetrygen
        image: ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:v0.74.0
        args: [traces, --otlp-endpoint=dev-collector:4317, --otlp-insecure, --duration=240s, --rate=4]
----
* Go to Observe -> Traces in the OpenShift web console.
* Select `<namespace>/simplest` from the `Tempo instance` dropdown and select `telemetrygen` from the `Filter by Service Name` dropdown.
* The traces from the above job is captured and displayed in the Traces UI. The `--otlp-endpoint` points to the OpenTelemetry Collector service endpoint, Example `dev-collector:4317` referred for the `collector.endpoint` in the <<_otel_collector_usage, helm command>>.

[[_tempo_tenant_configuration]]
==== Tempo Tenant Configuration
It is mandate to define and configure one or more tenants and their read and write access.

NOTE: Official documentation for link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/distributed_tracing/index#configuring-permissions-and-tenants_distr-tracing-tempo-installing[Configuring permissions and tenants]

===== Configuring read permissions for tenants
* To add the tenants to a cluster role with read permissions to read traces:
+
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tempostack-traces-reader
rules:
  - apiGroups:
      - 'tempo.grafana.com'
    resources:
      - dev
    resourceNames:
      - traces
    verbs:
      - 'get'
----
* To grant authenticated users the read permissions for trace data, you can create a cluster role binding for the above cluster role with the following,
+
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tempostack-traces-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tempostack-traces-reader
subjects:
  - kind: Group
    apiGroup: rbac.authorization.k8s.io
    name: system:authenticated
----

===== Configuring write permissions for tenants

* Create a Service Account for the OpenTelemetry Collector
+
[source, yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  # (1) Namespace for the Service Account
  namespace: <Namespace>
----
**References:**

. _Namespace for the Service Account:_ To keep the permissions and access control simple, the Service Account is deployed in the same namespace as the RHTPA installation. example `trustify`.

* Add the tenants to a cluster role with the write (create) permissions to write traces.
+
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tempostack-traces-write
rules:
  - apiGroups:
      - 'tempo.grafana.com'
    resources:
      - dev
    resourceNames:
      - traces
    verbs:
      - 'create'
----
* Grant the OpenTelemetry Collector the write permissions by defining a cluster role binding to attach the OpenTelemetry Collector service account
+
[source, yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tempostack-traces
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tempostack-traces-write

subjects:
  - kind: ServiceAccount
    name: otel-collector
    # (1) Namespace for the Service Account
    namespace: <Namespace>
----
+
**References:**

. _Namespace for the Service Account:_ Specify the service account namespace. example `trustify`.

* <<_otel_collector_installation,Configure OpenTelemetry collector>> custom resource with the tenant information.

[[_tempo_installation]]
==== Setting up Distributed Tracing for Tempo collector:

[[_prerequisites_tempo]]
==== Prerequisites

* Tempo requires Object storage to store traces data. You can use the  <<rhodf_operator_installation, Red Hat OpenShift Data Foundation>> to set up object storage, or you can use any other S3 compatible storage service like AWS S3, <<_minio_installation, minio>>, etc.


===== Installation
NOTE: Official documentation for link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/red_hat_build_of_opentelemetry/index#install-otel[Distributed tracing]

* Go to Operators -> OperatorHub in the OpenShift web console.
* Search for "Tempo" and select `Tempo Operator provided by Red Hat`
* On the model window, Click on `Install`
* On the `Install Operator` page, make sure to align the following options:
  ** **Update Channel**: `stable`
  ** **Installation Mode**: `All namespaces on the cluster (default)`
  ** **Installed Namespace**: `Operator recommended namespace: openshift-tempo-operator`
  ** **Update approval**: `Automatic`
* Click on `Install` to proceed with the installation.

===== Configuration
* Create a secret with S3 storage credentials. Refer the official documentation for setting up link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/distributed_tracing/index#distr-tracing-tempo-object-storage-setup_distr-tracing-tempo-installing[object storage setup]
** Sample code block for S3 credentials secret given below:
+
[source, yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: s3-secret
  namespace: <Namespace>
stringData:
  bucket: <bucket_name>
  endpoint: <storage_endpoint>
  access_key_id: <access_key_id>
  access_key_secret: <access_key_secret>
type: Opaque
----

* After the installation is complete, Go to the `Installed Operators` page.
* Select the RHTPA installation namespace, example `trustify`
* Click on the `Tempo Operator`, Select `TempoStack` and click on `Create TempoStack` on the Operator details page.
* The TempoStack requires a configuration file to define the storage and tracing settings for telemetry data. The below configuration is an example of the TempoStack,
+
[source, yaml]
----
apiVersion: tempo.grafana.com/v1alpha1
kind: TempoStack
metadata:
  name: simplest
  # (1) Namespace for the TempoStack
  namespace: <Namespace>
spec:
  storage:
    secret:
    # (2) Secret containing the S3 credentials
      name: s3-secret
      type: s3
  storageSize: 1Gi
  resources:
    total:
      limits:
        memory: 2Gi
        cpu: 2000m
  tenants:
    mode: openshift
    authentication:
      - tenantName: dev
      # (3) Unique UUID for the tenant
        tenantId: "1610b0c3-c509-4592-a256-a1871353dbfa"
      - tenantName: prod
        tenantId: "1610b0c3-c509-4592-a256-a1871353dbfb"
  template:
    gateway:
      enabled: true
    queryFrontend:
      jaegerQuery:
        enabled: true
----

**References:**

. _Namespace for the TempoStack:_ To keep the permissions and access control simple, TempoStack is deployed in the same namespace as the RHTPA installation. example `trustify`.
. _Secret containing the S3 credentials:_ The secret containing the S3 credentials for the object storage.
. _Unique UUID for the tenant:_ The unique UUID for the tenant is used to identify the tenant in the TempoStack. It is recommended to use a unique UUID for each tenant.

====== Distributed tracing UI Plugin:
* Go to Operators -> OperatorHub in the OpenShift web console.
* Search for `Cluster Observability Operator` and select `Cluster Observability Operator`
* On the model window, Click on `Install`
* Go to Operator -> Installed Operator
* Select `Cluster Observability Operator`, Select `UIPlugin` and Click `Create UIPlugin` on the operator details page. Use the below configuration to enable the Traces UI plugin.
+
[source, yaml]
----
apiVersion: observability.openshift.io/v1alpha1
kind: UIPlugin
metadata:
  name: distributed-tracing
spec:
  type: DistributedTracing
----
* After the installation, Refresh the web console
* `Traces` option will be available in the left navigation menu of the OpenShift web console under `Observe` section
* <<_otel_collector_configuration, Configure OTEL collector>> with the tempo tracing endpoint for the otlp.endpoint under the exporters section.

[[_minio_installation]]
==== MinIO Installation
Use the following steps to install MinIO on your OpenShift cluster to provide S3 compatible storage for TempoStack

[source, yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
spec:
  selector:
    matchLabels:
      app: minio
  serviceName: "minio"
  replicas: 1
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
        - name: minio
          image: quay.io/minio/minio:latest
          args:
            - server
            - /data
          env:
            - name: MINIO_ROOT_USER
              value: "minioadmin"
            - name: MINIO_ROOT_PASSWORD
              value: "minioadmin123"
          ports:
            - containerPort: 9000
          volumeMounts:
            - name: storage
              mountPath: /data
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi
----
Create route for the minio service
[source, yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: minio
spec:
  selector:
    app: minio
  ports:
    - protocol: TCP
      port: 9000
      targetPort: 9000
----
Create a S3 object storage bucket in MinIO
[source, bash]
----
export AWS_ACCESS_KEY_ID=minioadmin
export AWS_SECRET_ACCESS_KEY=minioadmin123
aws s3 mb s3://<bucketname> --endpoint-url <minio_endpoint>
----

[[_enable_monitoring]]
==== Enable Monitoring for User defined Projects
NOTE: Official documentation for link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/configuring-user-workload-monitoring#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm[Enabling Monitoring for User defined Projects]

* Create or Edit ConfigMap to enable User workload monitoring
+
[source, yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
----
* Enabling User workload monitoring add prometheus operator under `openshift-user-workload-monitoring` namespace.
+
[source, bash]
----
oc get prometheus -n openshift-user-workload-monitoring
----
* Create ServiceMonitor for the OTEL collector Services targeting the prometheus port and metrics port
+
[source, yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector-app-metrics
  # (1) Namespace for the ServiceMonitor
  namespace: <Namespace>
  labels:
    openshift.io/user-monitoring: "true"
    release: user-workload
spec:
  selector:
    matchLabels:
    # (2) Match labels for the OTEL collector service
      app.kubernetes.io/name: dev-collector
  endpoints:
  - port: prometheus
    interval: 30s
    path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector-self-metrics
  # (1) Namespace for the ServiceMonitor
  namespace: <Namespace>
  labels:
    openshift.io/user-monitoring: "true"
    release: user-workload
spec:
  selector:
    matchLabels:
    # (2) Match labels for the OTEL collector service
      app.kubernetes.io/name: dev-collector-monitoring
  endpoints:
  - port: metrics
    interval: 30s
----
+
**References:**

. _Namespace for the ServiceMonitor:_ To keep the permissions and access control simple, the ServiceMonitor is deployed in the same namespace as the RHTPA installation. example `trustify`
. _Match labels for the OTEL collector service:_ The labels used to match the OTEL collector service. The labels should match the labels used in the OTEL collector service.

* After ServiceMonitor creation, Go to Observe -> Metrics in the OpenShift web console.
* With the `collector.endpoint` pointing to the OTEL collector with <<_otel_collector_usage,helm installation>>, The metrics from the OTEL collector service displayed on the Metrics graph for RHTPA. Enter the expression `http_server_duration_seconds_bucket` in the query field and click on `Run Query` to display the metrics.
